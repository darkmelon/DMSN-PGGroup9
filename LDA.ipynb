{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd017767560fef6456e08416b833cd60cbe97e29174fd16fff4cbad564a6fa9df30",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "17767560fef6456e08416b833cd60cbe97e29174fd16fff4cbad564a6fa9df30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(328,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "posts = pd.read_csv('Cleaned Posts/PostsOnly-wallstreetbets-NOAutoMod.csv')\n",
    "comments = pd.read_csv('Cleaned Comments/CommentsOnly-wallstreetbets-NOAutoMod.csv')\n",
    "\n",
    "postsData = posts.copy()\n",
    "postsData['title'] = posts['title'].fillna('').astype(str) + \" \" + posts['selftext'].fillna('').astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# bodyComments = comments['body']\n",
    "# postsData = postsData['title']\n",
    "\n",
    "# print(bodyComments.shape)\n",
    "# print(postsData.shape)\n",
    "\n",
    "# subData = postsData.append(bodyComments)\n",
    "\n",
    "# print(subData.shape)\n",
    "\n",
    "\n",
    "documentArray = []\n",
    "\n",
    "\n",
    "linkIDs = comments['link_id'].unique().tolist()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for item in linkIDs:\n",
    "    returnString = \"\"\n",
    "    \n",
    "    \n",
    "    ###Get the comments of the specific post link_id\n",
    "    commentsofPost = comments[comments['link_id'] == item].fillna('').astype(str)\n",
    "    for index, row in commentsofPost.iterrows():\n",
    "        #Remove URLs from strings\n",
    "        singleComment = str(row['body'])\n",
    "        removedURL = re.sub(r\"http\\S+\", \"\", singleComment)\n",
    "        returnString += removedURL + \" \"\n",
    "    ################################################\n",
    "    \n",
    "    \n",
    "    ###Get Post and post body of post link_id \n",
    "    postTitleBody = postsData[postsData['id'] == item].fillna('').astype(str)\n",
    "    postString = str(postTitleBody.iloc[0]['title']) + \" \" + str(postTitleBody.iloc[0]['selftext'])\n",
    "    postString = re.sub(r\"http\\S+\", \"\", postString)\n",
    "    returnString = postString + returnString\n",
    "    ################################################\n",
    "\n",
    "    documentArray.append(str(returnString))\n",
    "\n",
    "#Convert to pandas\n",
    "subDocuments = pd.DataFrame(documentArray)\n",
    "subDocuments = subDocuments.squeeze()\n",
    "print(subDocuments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Nachiappan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nachiappan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "0    [hell, headlin, give, massiv, sticki, growth, ...\n",
      "1    [invest, bot, submiss, violat, content, guid, ...\n",
      "2    [sexual, identifi, wsb, young, lad, dream, soa...\n",
      "3    [weight, blanket, custom, commun, award, ticke...\n",
      "4    [haven, check, app, download, go, celebr, new,...\n",
      "5    [post, shiiit, nikola, sub, dirt, factori, nkl...\n",
      "6    [liter, tit, bot, submiss, violat, content, gu...\n",
      "7    [current, situat, bot, submiss, violat, conten...\n",
      "8    [nio, gain, bot, submiss, violat, content, gui...\n",
      "9    [wsb, reach, autism, new, space, forc, logo, p...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "my_stop_words = STOPWORDS.union(set(['like']))\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        # if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "        if token not in my_stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "# doc_sample = selfTextPosts.iloc[5]\n",
    "\n",
    "# print('original document: ')\n",
    "# words = []\n",
    "# for word in doc_sample.split(' '):\n",
    "#     words.append(word)\n",
    "# print(words)\n",
    "# print('\\n\\n tokenized and lemmatized document: ')\n",
    "# print(preprocess(doc_sample))\n",
    "\n",
    "\n",
    "\n",
    "processed_docs = subDocuments.map(preprocess)\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 bot\n",
      "1 give\n",
      "2 good\n",
      "3 growth\n",
      "4 headlin\n",
      "5 hell\n",
      "6 massiv\n",
      "7 sticki\n",
      "8 best\n",
      "9 buy\n",
      "10 call\n",
      "[(7, 2), (18, 2), (24, 1), (38, 1), (52, 1), (58, 3), (71, 1), (75, 1), (101, 1), (115, 1), (136, 1), (147, 2), (163, 1), (171, 1), (184, 1), (210, 2), (212, 1), (228, 1), (253, 1), (256, 1), (271, 1), (272, 2), (277, 1), (287, 1), (289, 1), (313, 2), (350, 1), (361, 1), (425, 2), (428, 1), (456, 1), (484, 1), (495, 1), (522, 1), (532, 1), (535, 1), (588, 1), (599, 1), (607, 1), (644, 1), (652, 1), (698, 1), (702, 1), (706, 1), (709, 1), (743, 1)]\n",
      "Word 7 (\"call\") appears 2 time.\n",
      "Word 18 (\"post\") appears 2 time.\n",
      "Word 24 (\"way\") appears 1 time.\n",
      "Word 38 (\"guy\") appears 1 time.\n",
      "Word 52 (\"retard\") appears 1 time.\n",
      "Word 58 (\"think\") appears 3 time.\n",
      "Word 71 (\"sell\") appears 1 time.\n",
      "Word 75 (\"go\") appears 1 time.\n",
      "Word 101 (\"hold\") appears 1 time.\n",
      "Word 115 (\"thank\") appears 1 time.\n",
      "Word 136 (\"open\") appears 1 time.\n",
      "Word 147 (\"week\") appears 2 time.\n",
      "Word 163 (\"posit\") appears 1 time.\n",
      "Word 171 (\"hope\") appears 1 time.\n",
      "Word 184 (\"watch\") appears 1 time.\n",
      "Word 210 (\"doubl\") appears 2 time.\n",
      "Word 212 (\"earlier\") appears 1 time.\n",
      "Word 228 (\"look\") appears 1 time.\n",
      "Word 253 (\"today\") appears 1 time.\n",
      "Word 256 (\"wish\") appears 1 time.\n",
      "Word 271 (\"crazi\") appears 1 time.\n",
      "Word 272 (\"damn\") appears 2 time.\n",
      "Word 277 (\"let\") appears 1 time.\n",
      "Word 287 (\"tesla\") appears 1 time.\n",
      "Word 289 (\"time\") appears 1 time.\n",
      "Word 313 (\"pre\") appears 2 time.\n",
      "Word 350 (\"jump\") appears 1 time.\n",
      "Word 361 (\"appar\") appears 1 time.\n",
      "Word 425 (\"earn\") appears 2 time.\n",
      "Word 428 (\"edit\") appears 1 time.\n",
      "Word 456 (\"happi\") appears 1 time.\n",
      "Word 484 (\"listen\") appears 1 time.\n",
      "Word 495 (\"mayb\") appears 1 time.\n",
      "Word 522 (\"pop\") appears 1 time.\n",
      "Word 532 (\"read\") appears 1 time.\n",
      "Word 535 (\"report\") appears 1 time.\n",
      "Word 588 (\"valu\") appears 1 time.\n",
      "Word 599 (\"ye\") appears 1 time.\n",
      "Word 607 (\"icln\") appears 1 time.\n",
      "Word 644 (\"late\") appears 1 time.\n",
      "Word 652 (\"bet\") appears 1 time.\n",
      "Word 698 (\"list\") appears 1 time.\n",
      "Word 702 (\"pictur\") appears 1 time.\n",
      "Word 706 (\"releas\") appears 1 time.\n",
      "Word 709 (\"spike\") appears 1 time.\n",
      "Word 743 (\"add\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(bow_corpus[41])\n",
    "\n",
    "\n",
    "bow_doc_80 = bow_corpus[41]\n",
    "for i in range(len(bow_doc_80)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_80[i][0], \n",
    "                                               dictionary[bow_doc_80[i][0]], bow_doc_80[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0.5318408789343217),\n (1, 0.375950112436695),\n (2, 0.15929160713668244),\n (3, 0.4329946640998139),\n (4, 0.39316170461752037),\n (5, 0.4564783359945488)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 3 \nWords: 0.024*\"money\" + 0.016*\"tax\" + 0.015*\"loan\" + 0.012*\"work\" + 0.012*\"year\" + 0.012*\"gain\" + 0.012*\"congrat\"\nTopic: 4 \nWords: 0.044*\"sell\" + 0.025*\"call\" + 0.018*\"hold\" + 0.018*\"share\" + 0.017*\"option\" + 0.015*\"gain\" + 0.014*\"plug\"\nTopic: 2 \nWords: 0.038*\"steel\" + 0.035*\"china\" + 0.020*\"product\" + 0.017*\"compani\" + 0.016*\"new\" + 0.014*\"stock\" + 0.014*\"covid\"\nTopic: 5 \nWords: 0.023*\"share\" + 0.013*\"go\" + 0.012*\"sell\" + 0.011*\"stock\" + 0.011*\"think\" + 0.011*\"get\" + 0.010*\"gme\"\nTopic: 0 \nWords: 0.028*\"tesla\" + 0.016*\"year\" + 0.016*\"compani\" + 0.014*\"money\" + 0.012*\"think\" + 0.012*\"elon\" + 0.012*\"stock\"\nTopic: 7 \nWords: 0.018*\"day\" + 0.017*\"market\" + 0.015*\"close\" + 0.015*\"call\" + 0.012*\"ban\" + 0.012*\"work\" + 0.011*\"time\"\nTopic: 1 \nWords: 0.019*\"peopl\" + 0.017*\"trump\" + 0.014*\"think\" + 0.012*\"shit\" + 0.012*\"get\" + 0.011*\"go\" + 0.010*\"right\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=20, iterations=400, workers=4)\n",
    "\n",
    "# print(lda_model.print_topics(num_topics=5, num_words=5))\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_topics=7, num_words=7):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 2 Word: 0.010*\"funni\" + 0.008*\"tesla\" + 0.006*\"launch\" + 0.004*\"know\" + 0.004*\"move\" + 0.004*\"leav\" + 0.001*\"pass\"\nTopic: 6 Word: 0.008*\"tank\" + 0.006*\"news\" + 0.005*\"stimulu\" + 0.005*\"gold\" + 0.005*\"repeat\" + 0.005*\"reddit\" + 0.004*\"ipo\"\nTopic: 4 Word: 0.009*\"tsla\" + 0.006*\"bud\" + 0.006*\"pl\" + 0.006*\"rock\" + 0.006*\"rip\" + 0.006*\"daddi\" + 0.005*\"exit\"\nTopic: 0 Word: 0.015*\"elon\" + 0.015*\"tesla\" + 0.011*\"earn\" + 0.010*\"ipo\" + 0.009*\"musk\" + 0.009*\"bezo\" + 0.009*\"papa\"\nTopic: 5 Word: 0.007*\"sell\" + 0.006*\"share\" + 0.006*\"call\" + 0.005*\"stock\" + 0.005*\"go\" + 0.005*\"think\" + 0.005*\"hold\"\nTopic: 1 Word: 0.010*\"vote\" + 0.008*\"click\" + 0.007*\"send\" + 0.007*\"messag\" + 0.006*\"bot\" + 0.006*\"remov\" + 0.005*\"exit\"\nTopic: 7 Word: 0.008*\"cent\" + 0.008*\"away\" + 0.008*\"eye\" + 0.007*\"close\" + 0.006*\"confus\" + 0.006*\"app\" + 0.006*\"sound\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=8, id2word=dictionary,passes=20, iterations=400, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_topics=7, num_words=7):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processed_docs[101])\n",
    "\n",
    "# for index, score in sorted(lda_model[bow_corpus[80]], key=lambda tup: -1*tup[1]):\n",
    "#     print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
